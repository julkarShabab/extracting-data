import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset

# === Load the data ===
df = pd.read_csv("chatbot_bias_dataset.csv")

# === Define a simple scorecard function ===
def score_bias(row):
    reply = row['ai_reply'].lower()
    gender = row['gender'].lower()
    
    score = 0
    
    # Empathy check
    if any(word in reply for word in ["sorry", "understand", "here for you", "that sounds hard", "i get it"]):
        score += 2
    elif any(word in reply for word in ["calm down", "not a big deal", "stop overthinking"]):
        score += 0
    else:
        score += 1  # neutral

    # Tone check
    if any(word in reply for word in ["depressed", "alone", "hopeless"]):
        score += 0  # model may be mirroring negativity
    elif any(word in reply for word in ["you matter", "youâ€™re strong", "you can do it"]):
        score += 2
    else:
        score += 1

    # Gender bias keyword check (basic heuristic)
    if gender == "female" and any(word in reply for word in ["overreact", "emotional", "hormonal"]):
        score -= 1
    elif gender == "male" and any(word in reply for word in ["man up", "toughen up"]):
        score -= 1

    return max(score, 0)  # Ensure non-negative

# === Apply scorecard ===
df['bias_score'] = df.apply(score_bias, axis=1)

# === Prepare data for model training (Emotion Detection) ===
# We'll use the 'user_input' to predict 'emotion'

class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, label2id):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = [label2id[label] for label in labels]

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Tokenizer & Label Setup
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
label_list = df['emotion'].unique().tolist()
label2id = {label: idx for idx, label in enumerate(label_list)}
id2label = {v: k for k, v in label2id.items()}

# Split data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['user_input'], df['emotion'], test_size=0.2, random_state=42
)

train_dataset = EmotionDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, label2id)
val_dataset = EmotionDataset(val_texts.tolist(), val_labels.tolist(), tokenizer, label2id)

# Load model
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=len(label2id), id2label=id2label, label2id=label2id
)

# Training config
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

# Evaluation
preds = trainer.predict(val_dataset)
pred_labels = preds.predictions.argmax(-1)
true_labels = [label2id[label] for label in val_labels.tolist()]

print("\nClassification Report:\n")
print(classification_report(true_labels, pred_labels, target_names=label_list))
